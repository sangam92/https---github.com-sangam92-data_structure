1.) Spark Applications consist of a driver process and a set of executor processes. 
    The driver process runs your main() function, sits on a node in the cluster,
    and is responsible for three things:maintaining information about the Spark Application; 
    responding to a user’s program or input;and analyzing, distributing, 
    and scheduling work across the executors (discussed momentarily).
    The driver process is absolutely essential—it’s the heart of a Spark Application 
    and maintains allrelevant information during the lifetime of the application.
    The executors are responsible for actually carrying out the work that the driver assigns them.
    This means that each executor is responsible for only two things: executing code assigned to itby the driver, 
    and reporting the state of the computation on that executor back to the driver node.

2.) Spark API 

    Spark has two fundamental sets of APIs:
    the low-level“unstructured” APIs, and the higher-level structured APIs.

3.) Data Frame 

    A DataFrame is the most common Structured API and simply represents a table of data with rows and columns. 
    The list that defines the columns and the types within those columns is called the schema

4.) Difference between Spark dataframes and python/R dataframes.

    The DataFrame concept is not unique to Spark. R and Python both have similar concepts.
    However, Python/R DataFrames (with some exceptions) exist on one machine rather than 
    multiple machines. This limits what you can do with a given DataFrame to the resources 
    that exist on that specific machine. However, because Spark has language interfaces for both 
    Python and R, it’s quite easy to convert Pandas (Python) DataFrames to Spark DataFrames,
    and R DataFrames to Spark DataFrames.

5.) Partitions

    To allow every executor to perform work in parallel, Spark breaks up the data into chunks called partitions.
    A partition is a collection of rows that sit on one physical machine in your cluster. 
    A DataFrame’s partitions represent how the data is physically distributed across the cluster of machines during execution.
    If you have one partition, Spark will have a parallelism of only one,even if you have thousands of executors.
    If you have many partitions but only one executor,Spark will 
    still have a parallelism of only one because there is only one computation resource.

6.) Transformation

    The core data structures are immutable, meaning they cannot be changed after they’recreated.
    This might seem like a strange concept at first: if you cannot change it, 
    how are you supposed to use it? To “change” a DataFrame, you need to instruct Spark 
    how you would like tomodify it to do what you want. 
    These instructions are called transformations. 
    Let’s perform asimple transformation to find all even numbers in our current DataFrame:

7.) Lazy Evaluation

    Lazy evaulation means that Spark will wait until the very last moment to execute the 
    graph of computation instructions. In Spark, instead of modifying the data immediately 
    when you express some operation, you build up a plan of transformations that you would 
    like to apply to your source data. By waiting until the last minute to execute the code, 
    Spark compiles this plan from your raw DataFrame transformations to a streamlined physical 
    plan that will run as efficiently as possible across the cluster. This provides immense 
    benefits because Spark can optimize theentire data flow from end to end.

8.) 